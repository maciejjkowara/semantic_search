{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maciejjkowara/semantic_search/blob/main/semantic_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d6c922-5529-4ba4-9809-30c968db8e0e",
      "metadata": {
        "id": "81d6c922-5529-4ba4-9809-30c968db8e0e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# !pip install sentence-transformers\n",
        "# !pip install faiss-cpu==1.7.4 --force-reinstall\n",
        "# !pip install numpy==1.24.3 --force-reinstall\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9035dd27-0d56-40ca-90c1-dd738911b9a9",
      "metadata": {
        "id": "9035dd27-0d56-40ca-90c1-dd738911b9a9"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# SEMANTIC SEARCH FOR FUND ANALYSES\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "import re\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "# Which asset class to search\n",
        "ASSET_CLASS = 'equity'  # Options: 'equity', 'fixed_income', 'allocation'\n",
        "\n",
        "# Number of results to return\n",
        "TOP_K = 5\n",
        "\n",
        "# ============================================\n",
        "# LOAD MODEL AND DATA\n",
        "# ============================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING MODEL AND EMBEDDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load the same model used for embedding\n",
        "print(\"\\nLoading sentence transformer model...\")\n",
        "embedder = SentenceTransformer(\n",
        "    'Alibaba-NLP/gte-base-en-v1.5',\n",
        "    trust_remote_code=True,\n",
        "    device='cpu'\n",
        ")\n",
        "embedder.max_seq_length = 4096\n",
        "print(\"✓ Model loaded!\")\n",
        "\n",
        "# # Load embeddings and FAISS index\n",
        "# print(f\"\\nLoading {ASSET_CLASS} embeddings and index...\")\n",
        "# embeddings = np.load(f'morningstar_embeddings_{ASSET_CLASS}_embeddings.npy')\n",
        "# index = faiss.read_index(f'morningstar_embeddings_{ASSET_CLASS}_index.faiss')\n",
        "# print(f\"✓ Loaded {len(embeddings)} embeddings\")\n",
        "\n",
        "# # Load metadata\n",
        "# print(f\"\\nLoading {ASSET_CLASS} metadata...\")\n",
        "# with open(f'{ASSET_CLASS}_analyses_metadata.json', 'r') as f:\n",
        "#     metadata = json.load(f)\n",
        "# print(f\"✓ Loaded metadata for {len(metadata)} analyses\")\n",
        "\n",
        "# # Load original text for display\n",
        "# print(f\"\\nLoading {ASSET_CLASS} analysis texts...\")\n",
        "# with open(f'{ASSET_CLASS}_analyses.txt', 'r', encoding='utf-8') as f:\n",
        "#     content = f.read()\n",
        "# analyses_texts = content.split('</analysis>')\n",
        "# analyses_texts = [a.strip() + '</analysis>' for a in analyses_texts\n",
        "#                   if a.strip() and '<analysis>' in a]\n",
        "# print(f\"✓ Loaded {len(analyses_texts)} analysis texts\")\n",
        "\n",
        "# ============================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def clean_text_for_embedding(text):\n",
        "    \"\"\"Remove metadata tags (same as embedding creation)\"\"\"\n",
        "    text = re.sub(r'<author>.*?</author>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'<date>.*?</date>', '', text, flags=re.DOTALL)\n",
        "    return text.strip()\n",
        "\n",
        "def extract_section(text, section_name):\n",
        "    \"\"\"Extract a specific section from analysis\"\"\"\n",
        "    pattern = f'<{section_name}>(.*?)</{section_name}>'\n",
        "    match = re.search(pattern, text, flags=re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return \"Not available\"\n",
        "\n",
        "def search_similar_analyses(query_text, top_k=5):\n",
        "    \"\"\"Search for similar analyses given a query description\"\"\"\n",
        "\n",
        "    # Clean and embed the query (same as we did for the analyses)\n",
        "    cleaned_query = clean_text_for_embedding(query_text)\n",
        "\n",
        "    print(f\"\\nEmbedding query...\")\n",
        "    query_embedding = embedder.encode(\n",
        "        [cleaned_query],\n",
        "        normalize_embeddings=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "\n",
        "    # Search FAISS index\n",
        "    print(f\"Searching for top {top_k} similar analyses...\")\n",
        "    distances, indices = index.search(query_embedding.astype('float32'), top_k)\n",
        "\n",
        "    # Compile results\n",
        "    results = []\n",
        "    for rank, (idx, distance) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "        result = {\n",
        "            'rank': rank,\n",
        "            'index': int(idx),\n",
        "            'distance': float(distance),\n",
        "            'similarity_score': float(1 / (1 + distance)),  # Convert distance to similarity\n",
        "            'fund_name': metadata[idx]['fund_name'],\n",
        "            'category': metadata[idx]['category'],\n",
        "            'asset_class': metadata[idx]['asset_class'],\n",
        "            'date': metadata[idx]['date'],\n",
        "            'author': metadata[idx]['author'],\n",
        "            'full_text': analyses_texts[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_results(results, query_text):\n",
        "    \"\"\"Pretty print search results with full analysis text\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SEARCH RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nQuery: {query_text[:200]}...\")\n",
        "    print(f\"\\nFound {len(results)} similar analyses:\\n\")\n",
        "\n",
        "    for r in results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"RESULT #{r['rank']}\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Similarity Score: {r['similarity_score']:.3f} | Distance: {r['distance']:.4f}\")\n",
        "        print(f\"Fund: {r['fund_name']}\")\n",
        "        print(f\"Category: {r['category']} | Asset Class: {r['asset_class']}\")\n",
        "        print(f\"Date: {r['date']} | Author: {r['author']}\")\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"FULL ANALYSIS:\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        # Extract and display each section\n",
        "        full_text = r['full_text']\n",
        "\n",
        "        print(\"\\n[SUMMARY]\")\n",
        "        print(extract_section(full_text, 'summary'))\n",
        "\n",
        "        print(\"\\n[PEOPLE]\")\n",
        "        print(extract_section(full_text, 'people'))\n",
        "\n",
        "        print(\"\\n[PROCESS]\")\n",
        "        print(extract_section(full_text, 'process'))\n",
        "\n",
        "        print(\"\\n[PORTFOLIO]\")\n",
        "        print(extract_section(full_text, 'portfolio'))\n",
        "\n",
        "        print(\"\\n[PERFORMANCE]\")\n",
        "        print(extract_section(full_text, 'performance'))\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0a8001-d840-4809-ac04-e707fff1d0f2",
      "metadata": {
        "id": "6c0a8001-d840-4809-ac04-e707fff1d0f2"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "Value, but not deep value.\n",
        "Large caps, dividend paying stocks.\n",
        "Definition of value somewhat subjective--not just valuation metrics, but value relative to growth potential.\n",
        "Historically has placed close to the border of large value and large blend boxes, typically megacaps.\n",
        "Most recent positioning: overweight financials, utilities; underweight materials.\n",
        "Process has been consistent for the past many years.\n",
        "Performance has been good relative to peers across most trailing period returns. But it failed to outperform the Russell 1000 Value index for the past 10 and 15 years.\n",
        "Management has been stable for years and the supporting team is well staffed and experienced.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0faf5048-7853-46c9-9974-ecea72236116",
      "metadata": {
        "id": "0faf5048-7853-46c9-9974-ecea72236116"
      },
      "outputs": [],
      "source": [
        "\n",
        "ASSET_CLASS = 'equity'\n",
        "\n",
        "# Reload the fixed income data\n",
        "embeddings = np.load(f'morningstar_embeddings_{ASSET_CLASS}_embeddings.npy')\n",
        "index = faiss.read_index(f'morningstar_embeddings_{ASSET_CLASS}_index.faiss')\n",
        "with open(f'{ASSET_CLASS}_analyses_metadata.json', 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "with open(f'{ASSET_CLASS}_analyses.txt', 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "analyses_texts = content.split('</analysis>')\n",
        "analyses_texts = [a.strip() + '</analysis>' for a in analyses_texts\n",
        "                  if a.strip() and '<analysis>' in a]\n",
        "\n",
        "results = search_similar_analyses(query, top_k=TOP_K)\n",
        "print_results(results, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39da3ef6-88b2-44d8-9a79-bb5e1fae6326",
      "metadata": {
        "id": "39da3ef6-88b2-44d8-9a79-bb5e1fae6326"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "file_history": [],
    "kernelspec": {
      "display_name": "morningstar-internal/analyticslab-ai:5611",
      "language": "python",
      "name": "conda-store://morningstar-internal/analyticslab-ai:5611"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}